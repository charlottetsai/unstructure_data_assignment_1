{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part_1_Text representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "# Load the file\n",
    "assignment_file = open('Assignment 1.csv',encoding=\"utf-8\")\n",
    "assignment_reader = csv.reader(assignment_file)\n",
    "review_data = list(assignment_reader)\n",
    "\n",
    "## 1. Tokenize each review in the collection\n",
    "tokenized_review_list =[]\n",
    "for review in review_data:\n",
    "    review_token = nltk.word_tokenize(review[1]) #Only fetch 2nd column\n",
    "    tokenized_review_list.append(review_token)\n",
    "print(tokenized_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert tokens into lowercase\n",
    "lower_tokenized_review_list =[]\n",
    "for sub_list in tokenized_review_list:\n",
    "    lower_tokenized_review_list.append(list(map(str.lower,sub_list)))\n",
    "\n",
    "#lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_lower_tokenized_review_list =[]\n",
    "for sub_list in lower_tokenized_review_list:\n",
    "    lemmatized_lower_tokenized_review_list.append([lemmatizer.lemmatize(token) for token in sub_list])\n",
    "print(lemmatized_lower_tokenized_review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop-words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_removed = []\n",
    "for sub_list in lemmatized_lower_tokenized_review_list:\n",
    "    result = [token for token in sub_list if not token in stopwords.words('english') if token.isalpha()] # remove punctuations\n",
    "    stop_words_removed.append(result)\n",
    "print(stop_words_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TF-IDF vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tdidf_review_list =[]\n",
    "for sub_list in stop_words_removed:\n",
    "    tdidf_review_list.append(\" \".join(sub_list))\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3)\n",
    "vectorizer.fit(tdidf_review_list)\n",
    "tfidfvectorized_review = vectorizer.transform(tdidf_review_list)\n",
    "print(tfidfvectorized_review.toarray())\n",
    "\n",
    "#Export to CSV file\n",
    "tdidf_review= tfidfvectorized_review.toarray()\n",
    "with open('assignment_1_solution_1_4.py.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerows(tdidf_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS TAG\n",
    "POS_token_review_list =[]\n",
    "for sub_list in tokenized_review_list:\n",
    "    POS_token_review = nltk.pos_tag(sub_list)\n",
    "    POS_token_temp = []\n",
    "    for i in POS_token_review:\n",
    "        POS_token_temp.append(i[0] + i[1])\n",
    "    POS_token_review_list.append(\" \".join(POS_token_temp))\n",
    "\n",
    "\n",
    "# ID-IDF vectorization\n",
    "vectorizer2 = TfidfVectorizer(min_df=4)\n",
    "vectorizer2.fit(POS_token_review_list)\n",
    "POS_vectorizer2 = vectorizer2.transform(POS_token_review_list)\n",
    "print(POS_vectorizer2.toarray())\n",
    "\n",
    "#Export to CSV file\n",
    "pos_review = POS_vectorizer2.toarray()\n",
    "with open('assignment_1_solution_1_5.py.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerows(pos_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2. Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the first 10 tokenized documents\n",
    "tokenized_review_list_10 = tokenized_review_list[:10]\n",
    "# do index-based encoding\n",
    "words = [word for sub_list in tokenized_review_list_10 for word in sub_list]\n",
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "index_encoder = LabelEncoder()\n",
    "encoded_index = index_encoder.fit(words)\n",
    "encoded_index_vector = [index_encoder.transform(sub_list).tolist() for sub_list in tokenized_review_list_10]\n",
    "\n",
    "#Export to CSV file\n",
    "\n",
    "with open('assignment_1_solution_2_1.py.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerows(encoded_index_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "index_list = [word for sub_list in encoded_index_vector for word in sub_list]\n",
    "# make each word a list\n",
    "index_list_vertical_array = array(index_list).reshape(-1,1)\n",
    "# Do one-hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoder = onehot_encoder.fit(index_list_vertical_array)\n",
    "onehot_encoded_list = [onehot_encoder.transform(array(doc_i).reshape(-1, 1)).tolist() for doc_i in encoded_index_vector]\n",
    "print(onehot_encoded_list)\n",
    "# Export to CSV file which has column A as review index, column B as word index of a review, column C for vector index of a word and column D for value\n",
    "with open('assignment_1_solution_2_2.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "    for review_index in range(0, len(onehot_encoded_list)):\n",
    "        for word_index in range(0, len(onehot_encoded_list[review_index])):\n",
    "            for word_vector_index in range(0, len(onehot_encoded_list[review_index][word_index])):\n",
    "                coordinate_and_value_list = []\n",
    "                coordinate_and_value_list.append(review_index)\n",
    "                coordinate_and_value_list.append(word_index)\n",
    "                coordinate_and_value_list.append(word_vector_index)\n",
    "                coordinate_and_value_list.append(onehot_encoded_list[review_index][word_index][word_vector_index])\n",
    "                writer.writerow(coordinate_and_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# Loading the glove file\n",
    "glove2word2vec(glove_input_file=\"glove.6B.50d.txt\",word2vec_output_file=\"glove.6B.50d.txt.word2vec\")\n",
    "from gensim.models import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"glove.6B.50d.txt.word2vec\",binary=False)\n",
    "# Remove undefined word and map the review with glove file\n",
    "glove_all_vector = []\n",
    "undefined_word = []\n",
    "for sub_list in tokenized_review_list_10:\n",
    "    glove_all_word_vector =[]\n",
    "    for word in sub_list:\n",
    "        try:\n",
    "            word_vector = glove_model[word]\n",
    "            glove_all_word_vector.append(word_vector)\n",
    "        except Exception:\n",
    "            undefined_word.append(word)\n",
    "    glove_all_vector.append(glove_all_word_vector)\n",
    "print(glove_all_vector)\n",
    "\n",
    "# Export to CSV file which has column A as review index, column B as word index of a review, column C for vector index of a word and column D for value\n",
    "with open('assignment_1_solution_2_3.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "    for review_index in range(0, len(glove_all_vector)):\n",
    "        for word_index in range(0, len(glove_all_vector[review_index])):\n",
    "            for word_vector_index in range(0, len(glove_all_vector[review_index][word_index])):\n",
    "                coordinate_and_value_list = []\n",
    "                coordinate_and_value_list.append(review_index)\n",
    "                coordinate_and_value_list.append(word_index)\n",
    "                coordinate_and_value_list.append(word_vector_index)\n",
    "                coordinate_and_value_list.append(glove_all_vector[review_index][word_index][word_vector_index])\n",
    "                writer.writerow(coordinate_and_value_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
